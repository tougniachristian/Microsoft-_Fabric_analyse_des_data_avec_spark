# üìò Analyser les donn√©es avec Apache Spark dans Fabric

Dans ce laboratoire, vous ing√©rerez des donn√©es dans le lac **Microsoft Fabric** et utiliserez **PySpark** pour lire et analyser les donn√©es.  

‚è±Ô∏è Ce laboratoire prendra environ **45 minutes** √† compl√©ter.  

> [!REMARQUE]  
> Vous devez avoir acc√®s √† un **locataire Microsoft Fabric** pour effectuer cet exercice.

---

## üß± Cr√©er un espace de travail

Avant de travailler avec des donn√©es dans Fabric, cr√©ez un espace de travail dans un locataire avec la capacit√© Fabric activ√©e.

1. Acc√©dez √† la page d‚Äôaccueil de Microsoft Fabric :  
   üëâ [https://app.fabric.microsoft.com/home?experience=fabric-developer](https://app.fabric.microsoft.com/home?experience=fabric-developer)
2. Connectez-vous avec vos informations d‚Äôidentification Fabric.  
3. Dans la barre de menu √† gauche, s√©lectionnez **Espaces de travail** (ic√¥ne üóá).  
4. Cr√©ez un nouvel espace de travail avec un **mode de licence Fabric** (Essai, Premium ou Fabric).  
5. Lorsque votre nouvel espace s‚Äôouvre, il doit √™tre **vide**.

üì∏ Capture d‚Äô√©cran :  
![Espace de travail vide](./images/1.png)

---

## üåä Cr√©er un Lakehouse et t√©l√©verser des fichiers

Maintenant que vous disposez d‚Äôun espace de travail, cr√©ons un **Lakehouse** pour stocker vos donn√©es.

1. Dans la barre de menu de gauche, s√©lectionnez **Cr√©er**.  
2. Sous **Ing√©nierie des donn√©es**, choisissez **Lakehouse** et donnez-lui un nom unique.  
   > Si l‚Äôoption *Cr√©er* n‚Äôest pas visible, cliquez sur les **‚Ä¶** pour la r√©v√©ler.
3. Apr√®s environ une minute, votre Lakehouse est cr√©√©.

üì∏ Capture d‚Äô√©cran :  
![Nouveau Lakehouse](./images/2.png)

4. Le volet d‚Äôexploration √† gauche vous permet de naviguer dans les **fichiers et tables** du Lakehouse.  

5. T√©l√©chargez les donn√©es depuis :  
   üëâ [https://github.com/MicrosoftLearning/dp-data/raw/main/orders.zip](https://github.com/MicrosoftLearning/dp-data/raw/main/orders.zip)  
   Extrayez le fichier ZIP pour obtenir :  

2019.csv
2020.csv
2021.csv


6. Dans le **Lakehouse**, ouvrez le menu **‚Ä¶ > T√©l√©charger > T√©l√©charger le dossier**, puis importez le dossier `orders`.

üì∏ Capture d‚Äô√©cran :  
![Fichiers CSV t√©l√©charg√©s](./images/3.png)

---

## üìì Cr√©er un carnet

1. Vous pouvez d√©sormais cr√©er un notebook Fabric pour exploiter vos donn√©es. Les notebooks offrent un environnement interactif o√π vous pouvez √©crire et ex√©cuter du code.

Dans la barre de menu de gauche, s√©lectionnez ¬´ Cr√©er ¬ª . Dans la page ¬´ Nouveau ¬ª , sous la section ¬´ Ing√©nierie des donn√©es ¬ª , s√©lectionnez ¬´ Notebook ¬ª .

üì∏ Capture d‚Äô√©cran :  
![Fichiers CSV t√©l√©charg√©s](./images/4.png)

2. Fabric attribue un nom √† chaque bloc-notes que vous cr√©ez, tel que Bloc-notes 1, Bloc-notes 2, etc. Cliquez sur le panneau de nom au-dessus de l' onglet Accueil dans le menu pour modifier le nom en quelque chose de plus descriptif.

3. S√©lectionnez la premi√®re cellule (actuellement une cellule de code), puis, dans la barre d'outils en haut √† droite, utilisez le bouton M‚Üì pour la convertir en cellule Markdown. Le texte contenu dans la cellule s'affichera alors sous forme de texte format√©.

4. Utilisez le bouton üñâ (Modifier) ‚Äã‚Äãpour passer la cellule en mode √©dition, puis modifiez le markdown comme indiqu√© ci-dessous.

üì∏ Capture d‚Äô√©cran :  
![Fichiers CSV t√©l√©charg√©s](./images/5.png)
Lorsque vous avez termin√©, cliquez n‚Äôimporte o√π dans le bloc-notes en dehors de la cellule pour arr√™ter la modification.

## Cr√©er un DataFrame


Maintenant que vous avez cr√©√© un espace de travail, un lakehouse et un notebook, vous √™tes pr√™t √† exploiter vos donn√©es. Vous utiliserez PySpark, le langage par d√©faut des notebooks Fabric, et la version de Python optimis√©e pour Spark.

1. S√©lectionnez votre nouvel espace de travail dans la barre de gauche. Vous verrez la liste des √©l√©ments qu'il contient, y compris votre Lakehouse et votre carnet.

2. S√©lectionnez le Lakehouse pour afficher le volet Explorateur, y compris le dossier des commandes .

3. Dans le menu sup√©rieur, s√©lectionnez ¬´ Ouvrir un carnet ¬ª , ¬´ Carnet existant ¬ª , puis ouvrez le carnet cr√©√© pr√©c√©demment. Il devrait maintenant √™tre ouvert √† c√¥t√© du volet Explorateur. D√©veloppez ¬´ Lakehouses ¬ª, puis la liste des fichiers, et s√©lectionnez le dossier ¬´ Commandes ¬ª. Les fichiers CSV que vous avez import√©s sont list√©s √† c√¥t√© de l'√©diteur de carnets, comme ceci :

üì∏ Capture d‚Äô√©cran :  
![Fichiers CSV t√©l√©charg√©s](./images/6.png)

4. Dans le menu‚Ä¶ du fichier 2019.csv, s√©lectionnez Charger les donn√©es > Spark . Le code suivant est automatiquement g√©n√©r√© dans une nouvelle cellule :

```python
Le code suivant est g√©n√©r√© :
df = spark.read.format("csv").option("header","true").load("Files/orders/2019.csv")
# df now is a Spark DataFrame containing CSV data from "Files/orders/2019.csv".
display(df)
```

1. S√©lectionnez ‚ñ∑ Ex√©cuter la cellule √† gauche de la cellule pour ex√©cuter le code.
2.Une fois le code de la cellule termin√©, examinez la sortie sous la cellule, qui devrait ressembler √† ceci :


üì∏ Capture d‚Äô√©cran :  
![Fichiers CSV t√©l√©charg√©s](./images/6.png)

2. Le r√©sultat affiche les donn√©es du fichier 2019.csv, affich√©es en colonnes et en lignes. Notez que les en-t√™tes de colonnes contiennent la premi√®re ligne de donn√©es. Pour corriger ce probl√®me, modifiez la premi√®re ligne du code comme suit :
```python
df = spark.read.format("csv").option("header","false").load("Files/orders/2019.csv")
```

3. Ex√©cutez √† nouveau le code afin que le DataFrame identifie correctement la premi√®re ligne comme donn√©e. Notez que les noms des colonnes sont d√©sormais _c0, _c1, etc.

4. Des noms de colonnes descriptifs vous aident √† donner du sens aux donn√©es. Pour cr√©er des noms de colonnes pertinents, vous devez d√©finir le sch√©ma et les types de donn√©es. Vous devez √©galement importer un ensemble standard de types Spark SQL pour d√©finir les types de donn√©es. Remplacez le code existant par le code suivant :
```python
from pyspark.sql.types import *

orderSchema = StructType([
    StructField("SalesOrderNumber", StringType()),
    StructField("SalesOrderLineNumber", IntegerType()),
    StructField("OrderDate", DateType()),
    StructField("CustomerName", StringType()),
    StructField("Email", StringType()),
    StructField("Item", StringType()),
    StructField("Quantity", IntegerType()),
    StructField("UnitPrice", FloatType()),
    StructField("Tax", FloatType())
])

df = spark.read.format("csv").schema(orderSchema).load("Files/orders/2019.csv")

display(df)
```

5. Ex√©cutez la cellule et examinez le r√©sultat :

üì∏ Capture d‚Äô√©cran :  
![Fichiers CSV t√©l√©charg√©s](./images/7.png)

6. Ce DataFrame inclut uniquement les donn√©es du fichier 2019.csv. Modifiez le code afin que le chemin d'acc√®s au fichier utilise le caract√®re g√©n√©rique * pour lire tous les fichiers du dossier des commandes :
```python
from pyspark.sql.types import *

orderSchema = StructType([
    StructField("SalesOrderNumber", StringType()),
    StructField("SalesOrderLineNumber", IntegerType()),
    StructField("OrderDate", DateType()),
    StructField("CustomerName", StringType()),
    StructField("Email", StringType()),
    StructField("Item", StringType()),
    StructField("Quantity", IntegerType()),
    StructField("UnitPrice", FloatType()),
    StructField("Tax", FloatType())
])

df = spark.read.format("csv").schema(orderSchema).load("Files/orders/*.csv")

display(df)
```

7. Lorsque vous ex√©cutez le code modifi√©, vous devriez voir les ventes pour 2019, 2020 et 2021. Seul un sous-ensemble des lignes est affich√©, vous ne verrez donc peut-√™tre pas de lignes pour chaque ann√©e.

## Explorer les donn√©es dans un DataFrame
L'objet DataFrame fournit des fonctionnalit√©s suppl√©mentaires telles que la possibilit√© de filtrer, de regrouper et de manipuler des donn√©es.

### Filtrer un DataFrame

1. Ajoutez une cellule de code en s√©lectionnant ¬´ + Code ¬ª qui appara√Æt lorsque vous passez la souris au-dessus ou en dessous de la cellule active ou de son r√©sultat. Vous pouvez √©galement s√©lectionner ¬´ Modifier ¬ª dans le menu du ruban, puis ¬´ + Ajouter une cellule de code ci-dessous .

2. Le code suivant filtre les donn√©es afin de ne renvoyer que deux colonnes. Il utilise √©galement count et distinct pour r√©sumer le nombre d'enregistrements :
```python

customers = df['CustomerName', 'Email']

print(customers.count())
print(customers.distinct().count())

display(customers.distinct())
```

3. Ex√©cutez le code et examinez le r√©sultat :

Le code cr√©e un nouveau DataFrame appel√© ¬´ clients ¬ª , contenant un sous-ensemble de colonnes du DataFrame df d'origine . Lors d'une transformation de DataFrame, le DataFrame d'origine n'est pas modifi√©, mais un nouveau DataFrame est renvoy√©.
Une autre fa√ßon d‚Äôobtenir le m√™me r√©sultat est d‚Äôutiliser la m√©thode select :

```python
customers = df.select("CustomerName", "Email")

```

Les fonctions DataFrame count et distinct sont utilis√©es pour fournir des totaux pour le nombre de clients et de clients uniques.

4. Modifiez la premi√®re ligne du code en utilisant select avec une fonction where comme suit :

```python
customers = df.select("CustomerName", "Email").where(df['Item']=='Road-250 Red, 52')
print(customers.count())
print(customers.distinct().count())

display(customers.distinct())
```

5. Ex√©cutez le code modifi√© pour s√©lectionner uniquement les clients ayant achet√© le produit Road-250 Red, 52. Notez que vous pouvez ¬´ cha√Æner ¬ª plusieurs fonctions afin que la sortie d'une fonction devienne l'entr√©e de la suivante. Dans ce cas, le DataFrame cr√©√© par la m√©thode select est le DataFrame source de la m√©thode where utilis√©e pour appliquer les crit√®res de filtrage.

## Agr√©ger et regrouper des donn√©es dans un DataFrame

1. Ajoutez une cellule de code et entrez le code suivant 

```python
productSales = df.select("Item", "Quantity").groupBy("Item").sum()

display(productSales)
```

2. Ex√©cutez le code. Vous pouvez constater que les r√©sultats affichent la somme des quantit√©s command√©es, regroup√©es par produit. La m√©thode groupBy regroupe les lignes par article, et la fonction d'agr√©gation sum qui suit est appliqu√©e aux colonnes num√©riques restantes (dans ce cas, Quantity ).

3. Ajoutez une autre cellule de code au bloc-notes et entrez le code suivant :

```python
from pyspark.sql.functions import *

yearlySales = df.select(year(col("OrderDate")).alias("Year")).groupBy("Year").count().orderBy("Year")

display(yearlySales)
```

4. Ex√©cutez la cellule. Examinez le r√©sultat. Les r√©sultats indiquent maintenant le nombre de commandes par an :

L' instruction d'importation vous permet d'utiliser la biblioth√®que Spark SQL.
La m√©thode select est utilis√©e avec une fonction ann√©e SQL pour extraire le composant ann√©e du champ OrderDate .
La m√©thode alias est utilis√©e pour attribuer un nom de colonne √† la valeur de l'ann√©e extraite.
La m√©thode groupBy regroupe les donn√©es par colonne Ann√©e d√©riv√©e.
Le nombre de lignes dans chaque groupe est calcul√© avant que la m√©thode orderBy ne soit utilis√©e pour trier le DataFrame r√©sultant.


üì∏ Capture d‚Äô√©cran :  
![Fichiers CSV t√©l√©charg√©s](./images/8.png)



## Utilisez Spark pour transformer des fichiers de donn√©es

1. Ajoutez une cellule de code au bloc-notes et saisissez ce qui suit :

```python
from pyspark.sql.functions import *

# Create Year and Month columns
transformed_df = df.withColumn("Year", year(col("OrderDate"))).withColumn("Month", month(col("OrderDate")))

# Create the new FirstName and LastName fields
transformed_df = transformed_df.withColumn("FirstName", split(col("CustomerName"), " ").getItem(0)).withColumn("LastName", split(col("CustomerName"), " ").getItem(1))

# Filter and reorder columns
transformed_df = transformed_df["SalesOrderNumber", "SalesOrderLineNumber", "OrderDate", "Year", "Month", "FirstName", "LastName", "Email", "Item", "Quantity", "UnitPrice", "Tax"]

# Display the first five orders
display(transformed_df.limit(5))
```


2. Ex√©cutez la cellule. Un nouveau DataFrame est cr√©√© √† partir des donn√©es de la commande d'origine, avec les transformations suivantes :

Colonnes Ann√©e et Mois ajout√©es, bas√©es sur la colonne OrderDate.
Colonnes FirstName et LastName ajout√©es, bas√©es sur la colonne CustomerName.
Les colonnes sont filtr√©es et r√©organis√©es, et la colonne CustomerName est supprim√©e.``

3. Examinez la sortie et v√©rifiez que les transformations ont √©t√© apport√©es aux donn√©es.

Vous pouvez utiliser la biblioth√®que Spark SQL pour transformer les donn√©es en filtrant les lignes, en d√©rivant, en supprimant, en renommant les colonnes et en appliquant d'autres modifications de donn√©es.



### Sauvegarder les donn√©es transform√©es
√Ä ce stade, vous souhaiterez peut-√™tre enregistrer les donn√©es transform√©es afin qu‚Äôelles puissent √™tre utilis√©es pour une analyse plus approfondie.

Parquet est un format de stockage de donn√©es populaire car il stocke efficacement les donn√©es et est pris en charge par la plupart des syst√®mes d'analyse de donn√©es √† grande √©chelle. En effet, la transformation des donn√©es n√©cessite parfois de les convertir d'un format, comme CSV, vers Parquet.

1. Pour enregistrer le DataFrame transform√© au format Parquet, ajoutez une cellule de code et ajoutez le code suivant :

```python
transformed_df.write.mode("overwrite").parquet('Files/transformed_data/orders')

print ("Transformed data saved!")
```


2. Ex√©cutez la cellule et attendez le message d'enregistrement des donn√©es. Ensuite, dans le volet Explorateur √† gauche, dans le menu ¬´ ‚Ä¶ ¬ª du n≈ìud ¬´ Fichiers ¬ª, s√©lectionnez ¬´ Actualiser ¬ª . S√©lectionnez le dossier ¬´ transformed_data ¬ª pour v√©rifier qu'il contient un nouveau dossier nomm√© ¬´ orders ¬ª, qui contient lui-m√™me un ou plusieurs fichiers Parquet.

3. Ajoutez une cellule avec le code suivant :

```python
orders_df = spark.read.format("parquet").load("Files/transformed_data/orders")
display(orders_df)
```

üì∏ Capture d‚Äô√©cran :  
![Fichiers CSV t√©l√©charg√©s](./images/9.png)



4. Ex√©cutez la cellule. Un nouveau DataFrame est cr√©√© √† partir des fichiers Parquet du dossier transformed_data/orders . V√©rifiez que les r√©sultats affichent les donn√©es de commande charg√©es depuis les fichiers Parquet.


### Enregistrer les donn√©es dans des fichiers partitionn√©s

1. Ajoutez une cellule avec du code pour enregistrer le dataframe, en partitionnant les donn√©es par ann√©e et par mois :

```python
orders_df.write.partitionBy("Year","Month").mode("overwrite").parquet("Files/partitioned_data")

print ("Transformed data saved!")
```

2. Ex√©cutez la cellule et attendez le message d'enregistrement des donn√©es. Ensuite, dans le volet Lakehouses √† gauche, dans le menu ¬´ ‚Ä¶ ¬ª du n≈ìud ¬´ Fichiers ¬ª, s√©lectionnez ¬´ Actualiser ¬ª et d√©veloppez le dossier partitioned_data pour v√©rifier qu'il contient une hi√©rarchie de dossiers nomm√©s ¬´ Ann√©e=xxxx ¬ª , chacun contenant des dossiers nomm√©s ¬´ Mois=xxxx ¬ª . Chaque dossier mensuel contient un fichier Parquet contenant les commandes du mois.

üì∏ Capture d‚Äô√©cran :  
![Fichiers CSV t√©l√©charg√©s](./images/11.png)


3. Ajoutez une nouvelle cellule avec le code suivant pour charger un nouveau DataFrame √† partir du fichier orders.parquet :

```python
orders_2021_df = spark.read.format("parquet").load("Files/partitioned_data/Year=2021/Month=*")

display(orders_2021_df)
```


4. Ex√©cutez la cellule et v√©rifiez que les r√©sultats affichent les donn√©es de commande pour les ventes en 2021. Notez que les colonnes de partitionnement sp√©cifi√©es dans le chemin (Ann√©e et Mois) ne sont pas incluses dans le DataFrame.



## Travailler avec des tables et SQL

Vous avez maintenant vu comment les m√©thodes natives de l'objet DataFrame permettent d'interroger et d'analyser les donn√©es d'un fichier. Cependant, vous serez peut-√™tre plus √† l'aise avec les tables utilisant la syntaxe SQL. Spark fournit un m√©tastore permettant de d√©finir des tables relationnelles.

La biblioth√®que Spark SQL prend en charge l'utilisation d'instructions SQL pour interroger les tables du m√©tastore. Cela offre la flexibilit√© d'un lac de donn√©es avec le sch√©ma de donn√©es structur√© et les requ√™tes SQL d'un entrep√¥t de donn√©es relationnel ‚Äì d'o√π le terme ¬´ data lakehouse ¬ª.

### Cr√©er un tableau

Les tables d'un m√©tastore Spark sont des abstractions relationnelles des fichiers du lac de donn√©es. Elles peuvent √™tre g√©r√©es par le m√©tastore ou en externe et g√©r√©es ind√©pendamment de celui-ci.

1. Ajoutez une cellule de code au bloc-notes et entrez le code suivant, qui enregistre le DataFrame des donn√©es de commande client sous forme de table nomm√©e salesorders :

```python
# Create a new table
df.write.format("delta").saveAsTable("salesorders")

# Get the table description
spark.sql("DESCRIBE EXTENDED salesorders").show(truncate=False)
```

1. Ex√©cutez la cellule de code et examinez la sortie, qui d√©crit la d√©finition de la nouvelle table.

2. Dans le volet Explorateur , dans le menu ¬´ ‚Ä¶ ¬ª du dossier ¬´ Tables ¬ª, s√©lectionnez ¬´ Actualiser ¬ª . D√©veloppez ensuite le n≈ìud ¬´ Tables ¬ª et v√©rifiez que la table ¬´ commandes ¬ª a bien √©t√© cr√©√©e.

üì∏ Capture d‚Äô√©cran :  
![Fichiers CSV t√©l√©charg√©s](./images/12.png)


Dans le menu ‚Ä¶ de la table des commandes, s√©lectionnez Charger les donn√©es > Spark . Une nouvelle cellule de code est ajout√©e, contenant un code similaire √† celui-ci :

```python
df = spark.sql("SELECT * FROM [your_lakehouse].salesorders LIMIT 1000")

display(df)
```

Ex√©cutez le nouveau code, qui utilise la biblioth√®que Spark SQL pour int√©grer une requ√™te SQL sur la table salesorder dans le code PySpark et chargez les r√©sultats de la requ√™te dans un DataFrame.

### Ex√©cuter le code SQL dans une cellule
Bien qu'il soit utile de pouvoir int√©grer des instructions SQL dans une cellule contenant du code PySpark, les analystes de donn√©es souhaitent souvent simplement travailler directement dans SQL.

1. Ajoutez une nouvelle cellule de code au bloc-notes et entrez le code suivant :

```sql
%%sql
SELECT YEAR(OrderDate) AS OrderYear,
       SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue
FROM salesorders
GROUP BY YEAR(OrderDate)
ORDER BY OrderYear;
```

2. Ex√©cutez la cellule et examinez les r√©sultats. Observez :

La commande %%sql au d√©but de la cellule (appel√©e magie) change le langage en Spark SQL au lieu de PySpark.
Le code SQL fait r√©f√©rence √† la table des commandes clients que vous avez cr√©√©e pr√©c√©demment.
La sortie de la requ√™te SQL s'affiche automatiquement comme r√©sultat sous la cellule.



## Visualisez les donn√©es avec Spark
Les graphiques vous permettent d'identifier des sch√©mas et des tendances plus rapidement qu'en analysant des milliers de lignes de donn√©es. Les notebooks Fabric int√®grent une vue graphique, mais celle-ci n'est pas con√ßue pour les graphiques complexes. Pour mieux contr√¥ler la cr√©ation de graphiques √† partir de donn√©es dans des DataFrames, utilisez des biblioth√®ques graphiques Python comme matplotlib ou seaborn .

### Afficher les r√©sultats sous forme de graphique

1. Ajoutez une nouvelle cellule de code et entrez le code suivant :

```sql
%%sql
SELECT * FROM salesorders
```

2. Ex√©cutez le code pour afficher les donn√©es de la vue des commandes client cr√©√©e pr√©c√©demment. Dans la section des r√©sultats sous la cellule, s√©lectionnez ¬´ + Nouveau graphique ¬ª .

3. Utilisez le bouton Cr√©er le mien en bas √† droite de la section des r√©sultats et d√©finissez les param√®tres du graphique :

Type de graphique : Graphique √† barres
Axe des X : Article
Axe Y : Quantit√©
Groupe de s√©ries : laisser vide
Agr√©gation : Somme
Valeurs manquantes et NULL : afficher comme 0
Empil√© : non s√©lectionn√©

4. Votre graphique devrait ressembler √† ceci :

üì∏ Capture d‚Äô√©cran :  
![Fichiers CSV t√©l√©charg√©s](./images/13.png)


D√©marrer avec matplotlib

1. Ajoutez une nouvelle cellule de code et entrez le code suivant :
```sql
sqlQuery = "SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear, \
                SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue, \
                COUNT(DISTINCT SalesOrderNumber) AS YearlyCounts \
            FROM salesorders \
            GROUP BY CAST(YEAR(OrderDate) AS CHAR(4)) \
            ORDER BY OrderYear"
df_spark = spark.sql(sqlQuery)
df_spark.show()
```
2. Ex√©cutez le code. Il renvoie un DataFrame Spark contenant le chiffre d'affaires annuel et le nombre de commandes. Pour visualiser les donn√©es sous forme de graphique, nous utiliserons d'abord la biblioth√®que Python matplotlib. Cette biblioth√®que est la biblioth√®que de tra√ßage principale sur laquelle s'appuient de nombreuses autres et offre une grande flexibilit√© pour la cr√©ation de graphiques.

3. Ajoutez une nouvelle cellule de code et ajoutez le code suivant :

```python
from matplotlib import pyplot as plt

# matplotlib requires a Pandas dataframe, not a Spark one
df_sales = df_spark.toPandas()

# Create a bar plot of revenue by year
plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'])

# Display the plot
plt.show()
```

4. Ex√©cutez la cellule et examinez les r√©sultats, qui se pr√©sentent sous forme d'un graphique √† colonnes indiquant le chiffre d'affaires brut total pour chaque ann√©e. Examinez le code et remarquez les points suivants :

La biblioth√®que matplotlib n√©cessite un Pandas DataFrame, vous devez donc convertir le Spark DataFrame renvoy√© par la requ√™te Spark SQL.
Au c≈ìur de la biblioth√®que matplotlib se trouve l' objet pyplot . Il constitue la base de la plupart des fonctionnalit√©s de tra√ßage.
Les param√®tres par d√©faut g√©n√®rent un graphique utilisable, mais il existe une marge de man≈ìuvre consid√©rable pour le personnaliser.

5. Modifiez le code pour tracer le graphique comme suit :

```python
from matplotlib import pyplot as plt

# Clear the plot area
plt.clf()

# Create a bar plot of revenue by year
plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')

# Customize the chart
plt.title('Revenue by Year')
plt.xlabel('Year')
plt.ylabel('Revenue')
plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)
plt.xticks(rotation=45)

# Show the figure
plt.show()
```

6. Ex√©cutez √† nouveau la cellule de code et visualisez les r√©sultats. Le graphique est d√©sormais plus compr√©hensible.

7. Un graphique est contenu dans une figure. Dans les exemples pr√©c√©dents, la figure a √©t√© cr√©√©e implicitement, mais elle peut l'√™tre explicitement. Modifiez le code pour tracer le graphique comme suit :
```python

from matplotlib import pyplot as plt

# Clear the plot area
plt.clf()

# Create a Figure
fig = plt.figure(figsize=(8,3))

# Create a bar plot of revenue by year
plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')

# Customize the chart
plt.title('Revenue by Year')
plt.xlabel('Year')
plt.ylabel('Revenue')
plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)
plt.xticks(rotation=45)

# Show the figure
plt.show()
```

8. Ex√©cutez √† nouveau la cellule de code et visualisez les r√©sultats. La figure d√©termine la forme et la taille du trac√©.

9. Une figure peut contenir plusieurs sous-graphiques, chacun sur son propre axe. Modifiez le code pour tracer le graphique comme suit :

```python
from matplotlib import pyplot as plt

# Clear the plot area
plt.clf()

# Create a figure for 2 subplots (1 row, 2 columns)
fig, ax = plt.subplots(1, 2, figsize = (10,4))

# Create a bar plot of revenue by year on the first axis
ax[0].bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')
ax[0].set_title('Revenue by Year')

# Create a pie chart of yearly order counts on the second axis
ax[1].pie(df_sales['YearlyCounts'])
ax[1].set_title('Orders per Year')
ax[1].legend(df_sales['OrderYear'])

# Add a title to the Figure
fig.suptitle('Sales Data')

# Show the figure
plt.show()
```

10. R√©ex√©cutez la cellule de code et affichez les r√©sultats.


‚úÖ R√©sum√©
Dans ce laboratoire, vous avez appris √† :
Cr√©er un Lakehouse dans Microsoft Fabric
Ingest, transformer et stocker des donn√©es avec PySpark
Cr√©er et interroger des Tables Delta
G√©n√©rer des visualisations avec Matplotlib et Seaborn
