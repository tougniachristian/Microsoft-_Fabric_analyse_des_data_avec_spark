# ğŸ“˜ Analyser les donnÃ©es avec Apache Spark dans Fabric

Dans ce laboratoire, vous ingÃ©rerez des donnÃ©es dans le lac **Microsoft Fabric** et utiliserez **PySpark** pour lire et analyser les donnÃ©es.  

â±ï¸ Ce laboratoire prendra environ **45 minutes** Ã  complÃ©ter.  

> [!REMARQUE]  
> Vous devez avoir accÃ¨s Ã  un **locataire Microsoft Fabric** pour effectuer cet exercice.

---

## ğŸ§± CrÃ©er un espace de travail

Avant de travailler avec des donnÃ©es dans Fabric, crÃ©ez un espace de travail dans un locataire avec la capacitÃ© Fabric activÃ©e.

1. AccÃ©dez Ã  la page dâ€™accueil de Microsoft Fabric :  
   ğŸ‘‰ [https://app.fabric.microsoft.com/home?experience=fabric-developer](https://app.fabric.microsoft.com/home?experience=fabric-developer)
2. Connectez-vous avec vos informations dâ€™identification Fabric.  
3. Dans la barre de menu Ã  gauche, sÃ©lectionnez **Espaces de travail** (icÃ´ne ğŸ—‡).  
4. CrÃ©ez un nouvel espace de travail avec un **mode de licence Fabric** (Essai, Premium ou Fabric).  
5. Lorsque votre nouvel espace sâ€™ouvre, il doit Ãªtre **vide**.

ğŸ“¸ Capture dâ€™Ã©cran :  
![Espace de travail vide](./images/1.png)

---

## ğŸŒŠ CrÃ©er un Lakehouse et tÃ©lÃ©verser des fichiers

Maintenant que vous disposez dâ€™un espace de travail, crÃ©ons un **Lakehouse** pour stocker vos donnÃ©es.

1. Dans la barre de menu de gauche, sÃ©lectionnez **CrÃ©er**.  
2. Sous **IngÃ©nierie des donnÃ©es**, choisissez **Lakehouse** et donnez-lui un nom unique.  
   > Si lâ€™option *CrÃ©er* nâ€™est pas visible, cliquez sur les **â€¦** pour la rÃ©vÃ©ler.
3. AprÃ¨s environ une minute, votre Lakehouse est crÃ©Ã©.

ğŸ“¸ Capture dâ€™Ã©cran :  
![Nouveau Lakehouse](./images/2.png)

4. Le volet dâ€™exploration Ã  gauche vous permet de naviguer dans les **fichiers et tables** du Lakehouse.  

5. TÃ©lÃ©chargez les donnÃ©es depuis :  
   ğŸ‘‰ [https://github.com/MicrosoftLearning/dp-data/raw/main/orders.zip](https://github.com/MicrosoftLearning/dp-data/raw/main/orders.zip)  
   Extrayez le fichier ZIP pour obtenir :  

2019.csv
2020.csv
2021.csv


6. Dans le **Lakehouse**, ouvrez le menu **â€¦ > TÃ©lÃ©charger > TÃ©lÃ©charger le dossier**, puis importez le dossier `orders`.

ğŸ“¸ Capture dâ€™Ã©cran :  
![Fichiers CSV tÃ©lÃ©chargÃ©s](./images/3.png)

---

## ğŸ““ CrÃ©er un Notebook

Les notebooks Fabric permettent dâ€™Ã©crire et dâ€™exÃ©cuter du code interactif.

1. Dans **IngÃ©nierie des donnÃ©es**, cliquez sur **CrÃ©er > Notebook**.  
2. Un nouveau notebook (par ex. *Bloc-notes 1*) sâ€™ouvre.  
3. Renommez-le (ex. : `Sales_Analysis`).  
4. Convertissez la premiÃ¨re cellule en **Markdown** et insÃ©rez le texte suivant :

```markdown
# Sales order data exploration
Use this notebook to explore sales order data



## ğŸ§® CrÃ©er un DataFrame

Vous allez utiliser **PySpark** pour lire et afficher les donnÃ©es CSV.

1. Ouvrez votre **Lakehouse** dans le volet **Explorateur**.  
2. Dans le menu du fichier `2019.csv`, sÃ©lectionnez **Charger les donnÃ©es > Spark**.  

Le code suivant est gÃ©nÃ©rÃ© :

```python
df = spark.read.format("csv").option("header","true").load("Files/orders/2019.csv")
display(df)
ExÃ©cutez la cellule pour afficher les donnÃ©es.
Si la premiÃ¨re ligne est considÃ©rÃ©e comme donnÃ©e, corrigez le code :
df = spark.read.format("csv").option("header","false").load("Files/orders/2019.csv")
DÃ©finissez ensuite un schÃ©ma explicite pour les colonnes :
from pyspark.sql.types import *

orderSchema = StructType([
    StructField("SalesOrderNumber", StringType()),
    StructField("SalesOrderLineNumber", IntegerType()),
    StructField("OrderDate", DateType()),
    StructField("CustomerName", StringType()),
    StructField("Email", StringType()),
    StructField("Item", StringType()),
    StructField("Quantity", IntegerType()),
    StructField("UnitPrice", FloatType()),
    StructField("Tax", FloatType())
])

df = spark.read.format("csv").schema(orderSchema).load("Files/orders/*.csv")
display(df)
ğŸ“¸ Capture dâ€™Ã©cran :
ğŸ” Explorer les donnÃ©es
Filtrer un DataFrame
customers = df.select("CustomerName", "Email").where(df['Item']=='Road-250 Red, 52')
print(customers.count())
print(customers.distinct().count())
display(customers.distinct())
AgrÃ©ger et regrouper les donnÃ©es
from pyspark.sql.functions import *

productSales = df.select("Item", "Quantity").groupBy("Item").sum()
display(productSales)

yearlySales = df.select(year(col("OrderDate")).alias("Year")).groupBy("Year").count().orderBy("Year")
display(yearlySales)
ğŸ“¸ Capture dâ€™Ã©cran :
ğŸ”„ Transformer les donnÃ©es
from pyspark.sql.functions import *

transformed_df = df.withColumn("Year", year(col("OrderDate"))) \
                   .withColumn("Month", month(col("OrderDate"))) \
                   .withColumn("FirstName", split(col("CustomerName"), " ").getItem(0)) \
                   .withColumn("LastName", split(col("CustomerName"), " ").getItem(1)) \
                   .select("SalesOrderNumber", "SalesOrderLineNumber", "OrderDate", "Year", "Month", 
                           "FirstName", "LastName", "Email", "Item", "Quantity", "UnitPrice", "Tax")

display(transformed_df.limit(5))
ğŸ“¸ Capture dâ€™Ã©cran :
ğŸ’¾ Sauvegarder les donnÃ©es transformÃ©es
Enregistrer au format Parquet
transformed_df.write.mode("overwrite").parquet('Files/transformed_data/orders')
ğŸ“¸ Capture dâ€™Ã©cran :
Partitionner les donnÃ©es
orders_df = spark.read.format("parquet").load("Files/transformed_data/orders")
orders_df.write.partitionBy("Year","Month").mode("overwrite").parquet("Files/partitioned_data")
ğŸ“¸ Capture dâ€™Ã©cran :
ğŸ§© Travailler avec des Tables et SQL
CrÃ©er une table Delta
df.write.format("delta").saveAsTable("salesorders")
spark.sql("DESCRIBE EXTENDED salesorders").show(truncate=False)
ğŸ“¸ Capture dâ€™Ã©cran :
Interroger avec Spark SQL
%%sql
SELECT YEAR(OrderDate) AS OrderYear,
       SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue
FROM salesorders
GROUP BY YEAR(OrderDate)
ORDER BY OrderYear;
ğŸ“¸ Capture dâ€™Ã©cran :
ğŸ“Š Visualiser les donnÃ©es
Vue graphique intÃ©grÃ©e
ğŸ“¸ Capture dâ€™Ã©cran :
Matplotlib
from matplotlib import pyplot as plt

df_spark = spark.sql("""
SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear,
       SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue,
       COUNT(DISTINCT SalesOrderNumber) AS YearlyCounts
FROM salesorders
GROUP BY CAST(YEAR(OrderDate) AS CHAR(4))
ORDER BY OrderYear
""").toPandas()

plt.bar(x=df_spark['OrderYear'], height=df_spark['GrossRevenue'], color='orange')
plt.title('Revenue by Year')
plt.xlabel('Year')
plt.ylabel('Revenue')
plt.show()
ğŸ“¸ Capture dâ€™Ã©cran :
Seaborn
import seaborn as sns
plt.clf()
sns.set_theme(style="whitegrid")
sns.lineplot(x="OrderYear", y="GrossRevenue", data=df_spark)
plt.show()
ğŸ“¸ Capture dâ€™Ã©cran :
âœ… RÃ©sumÃ©
Dans ce laboratoire, vous avez appris Ã  :
CrÃ©er un Lakehouse dans Microsoft Fabric
Ingest, transformer et stocker des donnÃ©es avec PySpark
CrÃ©er et interroger des Tables Delta
GÃ©nÃ©rer des visualisations avec Matplotlib et Seaborn
